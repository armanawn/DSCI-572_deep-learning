{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/dsci572_header.png\" width=\"600\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1: Floating-Point Numbers, Optimization & Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "<hr>\n",
    "\n",
    "rubric={mechanics:3}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4aecf0223bc592cc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "- Follow the [general lab instructions](https://ubc-mds.github.io/resources_pages/general_lab_instructions/)\n",
    "- Upload a PDF version of your lab notebook to Gradescope, in addition to your `.ipynb` file.\n",
    "- Add a link to your GitHub repository here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to export DSCI 572 lab notebook to PDF:\n",
    "\n",
    "- In your Conda environment, run the following two commands:\n",
    "\n",
    "```\n",
    "pip install -U kaleido\n",
    "pip install pyppeteer\n",
    "```\n",
    "\n",
    "If you’re on Windows, install `kaleido` using the following command instead:\n",
    "\n",
    "```\n",
    "pip install -U \"kaleido==0.1.*\"\n",
    "```\n",
    "\n",
    "- In the import cell at the top of your lab notebook, add the following two lines:\n",
    "\n",
    "```\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"png\"\n",
    "```\n",
    "\n",
    "- Restart the kernel, run all cells from top to bottom, and save your notebook. Since some cells might intentionally run into error and stop the notebook execution, make sure that all cells have actually been run.\n",
    "\n",
    "- Navigate to the lab folder, and run `jupyter nbconvert --to webpdf lab1.ipynb` in your Conda environment.\n",
    "\n",
    "You should now be able to see both `matplotlib` and `plotly` figures in the output PDF. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "from scipy.optimize import minimize\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, log_loss\n",
    "from utils.floating_point import *\n",
    "\n",
    "%config InlineBackend.figure_formats = ['svg']\n",
    "plt.rcParams.update({'font.size': 12, 'axes.labelweight': 'bold'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Floating Point Numbers\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you're going to explore floating point behaviour. You'll be provided with a code snippet which you should run and then you need to explain the result. Below are three example snippets and answers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sample Code Snippet 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.3 - 0.2 - 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sample Answer**: The result is not zero because 0.3, 0.2, and 0.1 are not represented exactly as floating point numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sample Code Snippet 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.5 - 0.25 - 0.125 - 0.125"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sample Answer**: The result is zero because 0.5 ($2^{-1}$), 0.25 ($2^{-2}$), and 0.125 ($2^{-3}$) are powers of 2 and they _are_ represented exactly as floating point numbers. There is no round-off error present. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sample Code Snippet 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.4 - 0.2 - 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sample Answer**: The result is correct (zero) despite the fact that 0.4 and 0.2 are not represented exactly as floating point numbers. This is a case of good luck: while 0.4 and 0.2 are not represented exactly, the roundoff errors happened to cancel out during the subtractions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hint:** You can determine if a number can be exactly represented using a function I made called `float_rep()` from `utils.floating_point` that we imported at the top of the notebook. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "float_rep(0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "float_rep(0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "float_rep(0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1\n",
    "rubric={reasoning:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "30 - 20 - 10"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Your answer goes here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2\n",
    "rubric={reasoning:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "30.0 - 20.0 - 10.0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Your answer goes here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3\n",
    "rubric={reasoning:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(10.0 ** 100 + 1) == 10.0 ** 100"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Your answer goes here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4\n",
    "rubric={reasoning:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "(10.0 ** 100000 + 1) - 10.0 ** 100000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Your answer goes here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5\n",
    "rubric={reasoning:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(1000) - np.exp(1000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Your answer goes here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6\n",
    "rubric={reasoning:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 / np.exp(1000) == np.exp(-1000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Your answer goes here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7\n",
    "rubric={reasoning:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(1000) == np.exp(10000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Your answer goes here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8\n",
    "rubric={reasoning:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.ones(100000)\n",
    "x[0] = 1e20\n",
    "\n",
    "y = np.ones(100000)\n",
    "y[-1] = 1e20\n",
    "\n",
    "sum(x) == sum(y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Your answer goes here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: logpdf\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1\n",
    "rubric={reasoning:2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In most scientific computing libraries there are functions to compute the probability density function (pdf) of a probability distribution, and separate functions to compute the log of the pdf. For example, looking at the [R normal distribution documentation](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/Normal.html), you will see that you can set `log=TRUE` or `log=FALSE`. Likewise in Python we have `scipy.stats.norm.pdf` and `scipy.stats.norm.logpdf`.\n",
    "\n",
    "You might wonder why we bother creating these extra functions, when we can just take the log ourselves. For example, why do we need this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.norm.logpdf(1)  # is this function useless?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we can do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(scipy.stats.norm.pdf(1))  # seems to do the same thing as above?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do we need these \"extra\" functions for computing the log of a PDF?\n",
    "\n",
    "**Hint**: Try using a larger number in the functions above and see what happens and if you can explain the result."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Your answer goes here, replacing this text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 (OPTIONAL)\n",
    "rubric={accuracy:1}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you show how large `x` can be before `scipy.stats.norm.pdf(x)` underflows?\n",
    "\n",
    "**Hint:** You'll need the value of the smallest 64-bit floating-point number."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Your answer goes here, replacing this text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3\n",
    "rubric={accuracy:1}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a numerically safe function `log_gaussian_pdf` that computes the log of the standard Gaussian PDF (zero mean, unit variance).\n",
    "\n",
    "Show that your function produces the same result as `scipy.stats.norm.logpdf` when evaluated at $x=100$.\n",
    "\n",
    "**Hint**: Try working out the log of the [Gaussian PDF](https://en.wikipedia.org/wiki/Normal_distribution) by hand and coding that up directly. Sometimes re-arranging a formula directly can help avoid overflow errors."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Your answer goes here, replacing this text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_gaussian_pdf(x):\n",
    "    ...\n",
    "\n",
    "\n",
    "assert log_gaussian_pdf(100) == scipy.stats.norm.logpdf(100), \"Result not the same as scipy.stats.norm.logpdf\"\n",
    "print(\"Passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Gradient Descent Learning Rate\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the same functions we saw in Lecture 2 for calculating the MSE and gradient of the MSE for a linear regression model $\\hat{y}=\\boldsymbol{w^T}\\boldsymbol{x}$, and a function for doing gradient descent. Note that the gradient descent function returns both the final optimized `w` and a list of the losses at each iteration (you'll need this \"loss history\" for a later question)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(w, X, y):\n",
    "    \"\"\"Mean squared error.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    w : ndarray\n",
    "        Weight vector.\n",
    "    X : ndarray\n",
    "        Feature data.\n",
    "    y : ndarray\n",
    "        Response data.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    ndarray\n",
    "        Mean squared error.\n",
    "    \"\"\"\n",
    "    return np.mean((X @ w - y) ** 2)\n",
    "\n",
    "\n",
    "def mse_grad(w, X, y):\n",
    "    \"\"\"Gradient of mean squared error.\n",
    "        \n",
    "    Parameters\n",
    "    ----------\n",
    "    w : ndarray\n",
    "        Weight vector.\n",
    "    X : ndarray\n",
    "        Feature data.\n",
    "    y : ndarray\n",
    "        Response data.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    ndarray\n",
    "        Gradient of MSE loss w.r.t parameters w.\n",
    "    \"\"\"\n",
    "    return X.T @ (X @ w) - X.T @ y\n",
    "\n",
    "\n",
    "def gradient_descent(f, f_grad, w, X, y, alpha, ϵ=2e-4, max_iterations=500, verbose=False):\n",
    "    \"\"\"Gradient descent.\n",
    "    \n",
    "        \n",
    "    Parameters\n",
    "    ----------\n",
    "    f : ndarray\n",
    "        Function to minimize.\n",
    "    f_grad : ndarray\n",
    "        Gradient of function w.r.t weights.\n",
    "    w : ndarray\n",
    "        Weight vector.\n",
    "    X : ndarray\n",
    "        Feature data.\n",
    "    y : ndarray\n",
    "        Response data.\n",
    "    alpha : float\n",
    "        Learning rate.\n",
    "    ϵ : float\n",
    "        Stop GD if step-size smaller than this.\n",
    "    max_iterations : int\n",
    "        Maximum iterations of GD to do.\n",
    "    verbose : bool\n",
    "        Whether to print progress.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    ndarray\n",
    "        Mean squared error.\n",
    "    \"\"\"\n",
    "\n",
    "    iterations = 1         # init. iterations\n",
    "    dw = np.array(2 * ϵ)   # init. dw\n",
    "    losses = [f(w, X, y)]  # init. loss history \n",
    "    while abs(dw).sum() > ϵ and iterations <= max_iterations:\n",
    "        g = f_grad(w, X, y)  # calculate current gradient\n",
    "        dw = alpha * g       # change in w\n",
    "        w = w - dw           # adjust w based on gradient * learning rate\n",
    "        iterations += 1      # increase iteration\n",
    "        losses.append(f(w, X, y))\n",
    "        if verbose and iterations % 50 == 0: print(f\"Loss: {f(w, X, y):.4f}\")\n",
    "    if verbose: print(f\"Terminated after {iterations - 1} iterations!\")\n",
    "    return w, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a simple regression dataset to test our gradient descent function. Our model is going to be a simple linear regression model, $\\hat{y_i}=wx_i$, with just one parameter to fit, $w$, and no intercept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_regression(100, 1, noise=10, random_state=2020)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fit our simple linear model $\\hat{y_i}=wx_i$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)  # fix the random seed for reproducibility\n",
    "w0 = np.random.randint(0, 20, 1)  # random intial weight\n",
    "w, losses = gradient_descent(mse, mse_grad, w0, X, y, alpha=0.001, verbose=True)\n",
    "print(f\"Initial w = {w0[0]:.2f}\")\n",
    "print(f\"Optimum w = {w[0]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To confirm everything is working as expected, let's compare our result to scikit learn's `LinearRegression`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression(fit_intercept=False).fit(X, y)\n",
    "print(f\"w = {lr.coef_[0]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1\n",
    "rubric={viz:4}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw in Lecture 2, in the case of linear regression, the MSE loss is [convex](https://en.wikipedia.org/wiki/Convex_function), that basically means that there is a \"global minimum\" that gradient descent will always be able to find (if we do enough iterations and the learning rate is not too high that the gradients explode). This is what the \"loss surface\" looks like - i.e., the loss function for different values of $w$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't worry too much about this plotting code, it's just for illustration purposes\n",
    "weights = np.arange(0, 41, 1)\n",
    "loss = [mse(np.array([_]), X, y) for _ in weights]\n",
    "w, losses = gradient_descent(mse, mse_grad, w0, X, y, alpha=0.001, verbose=False)\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=weights, y=loss, name=\"Loss\"))\n",
    "fig.add_trace(go.Scatter(x=w0, y=losses[:1], mode=\"markers\", marker=dict(size=14, line=dict(width=1, color=\"DarkSlateGrey\")), name=\"Initial w\"))\n",
    "fig.add_trace(go.Scatter(x=w, y=losses[-1:], mode=\"markers\", marker=dict(size=14, line=dict(width=1, color=\"DarkSlateGrey\")), name=\"Optimal w\"))\n",
    "fig.update_layout(width=600, height=400, margin=dict(t=30)); fig.update_xaxes(title=\"w\", dtick=2); fig.update_yaxes(title=\"MSE loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how it's convex? Remember with gradient descent we are iteratively taking steps \"down hill\" so it makes sense that with each iteration of gradient descent (i.e., each \"step\" down hill), our loss *should* go down and move us towards that \"global minimum\" (assuming our learning rate is not too high).\n",
    "\n",
    "Using the gradient descent code and data we defined at the beginning of the exercise, I want you to show that loss decreases with iterations by making a plot of `loss` vs `iterations` for the following five learning rates: 0.00001, 0.00005, 0.0001, 0.001, 0.022. Your plot should look something like the below and have 5 lines on it (**you don't have to use plotly though, use any plotting library you like!**):\n",
    "\n",
    "![](img/plot.png)\n",
    "\n",
    "**Hint:** I've coded our function above so that it returns the losses. For example, to get the losses per iteration for `α=0.00001`, run: `w, losses = gradient_descent(mse, mse_grad, w0, X, y, alpha=0.00001)` and you can then plot `losses`. It doesn't matter what `w0` you use for this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2\n",
    "rubric={reasoning:2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all went well, you probably saw in the plot above that as learning rate increases, the number of iterations to convergence typically decreases—makes sense, a higher learning rate means we are taking bigger steps towards the minimum. **However** the final learning rate of 0.022 actually converged slower than a rate of 0.001. Just in case your plot didn't show this, we can confirm by running our code again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_descent(mse, mse_grad, w0, X, y, alpha=0.001, verbose=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_descent(mse, mse_grad, w0, X, y, alpha=0.022, verbose=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why does a learning rate of 0.022 converge slower here?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Your answer goes here, replacing this text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3\n",
    "rubric={reasoning:2}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the data and simple linear regression model we are using in this exercise, answer the following:\n",
    "\n",
    "1. Does the number of iterations it takes gradient descent to converge depend on the initial weight (`w0`) we specify?\n",
    "\n",
    "2. Does gradient descent always converge to the same optimum `w` regardless of the initial weight (`w0`) value?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Your answer goes here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4\n",
    "rubric={reasoning:2}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next few lectures we'll get to neural networks which are typically non-linear models. This makes them harder to optimize, especially with gradient descent.\n",
    "\n",
    "Non-linearities in our model lead to loss surfaces that are no longer convex. They are bumpy and have what we call \"local minima\" which we can get stuck in. Gradient descent is particularly prone to getting stuck in local minimum. We'll explore what that means in this exercise.\n",
    "\n",
    "Imagine we now want to fit a non-linear model, $\\hat{y_i} = w\\cos(wx_i)$, to our dataset (rather than our previous linear model $\\hat{y_i}=wx_i$). I've coded up that function (`nonlin`), the mse loss function (`mse_nonlin`), and the gradient of the loss function  (`mse_nonlin_grad`) below, along with a plotting function (I'm omitting docstrings now to be concise). Run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_regression(100, 1, random_state=2020)\n",
    "X = X.flatten()\n",
    "\n",
    "def nonlin(w, x):\n",
    "    return w * np.cos(w * x)\n",
    "\n",
    "def mse_nonlinear(w, x, y):\n",
    "    \"\"\"Mean squared error.\"\"\"\n",
    "    return np.mean((nonlin(w, x) - y) ** 2)\n",
    "\n",
    "def mse_nonlinear_grad(w, x, y):\n",
    "    \"\"\"Gradient of mean squared error.\"\"\"\n",
    "    t = np.cos(w * x) - w * x * np.sin(w * x)\n",
    "    return np.mean((nonlin(w, x) - y) * t)\n",
    "\n",
    "def plot_loss_vs_weight(w0, alpha=0.01):\n",
    "    \"\"\"Plotting function using plotly - don't worry too much about the code here!\"\"\"\n",
    "    if abs(w0) > 10: raise ValueError(\"Please specify w0 between -10 and 10\")\n",
    "    wf = gradient_descent(mse_nonlinear, mse_nonlinear_grad, w0, X, y, alpha=alpha, ϵ=2e-5, verbose=False)[0];\n",
    "    weights = np.arange(-10, 10.1, 0.1)\n",
    "    loss = [mse_nonlinear(_, X, y) for _ in weights]\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=weights, y=loss, name=\"Loss\"))\n",
    "    fig.add_trace(go.Scatter(x=[w0], y=[mse_nonlinear(w0, X, y)], mode=\"markers\", marker=dict(size=14, line=dict(width=1, color=\"DarkSlateGrey\")), name=\"Initial W\"))\n",
    "    fig.add_trace(go.Scatter(x=[wf], y=[mse_nonlinear(wf, X, y)], mode=\"markers\", marker=dict(size=14, line=dict(width=1, color=\"DarkSlateGrey\")), name=\"Final W\"))\n",
    "    fig.update_layout(width=600, height=500)\n",
    "    fig.update_xaxes(title=\"w\", dtick=2)\n",
    "    fig.update_yaxes(title=\"loss\")\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go ahead and plot this loss surface for different weights. Our plotting function also takes in an initial weight, `w0` and uses gradient descent to find the optimal weight and plots those too. Run the cell below to see what I mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0 = 9\n",
    "plot_loss_vs_weight(w0, alpha=0.01)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how our non-linear model results in this non-linear loss surface? It's very different to our nice convex one from earlier! Play around with the inital weight `w0` and answer the following:\n",
    "\n",
    "1. Does the number of iterations it takes gradient descent to converge depend on the initial weight (`w0`)?\n",
    "\n",
    "2. Does gradient descent always converge to the same optimum `w` regardless of the initial weight (`w0`) value?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Your answer goes here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5\n",
    "rubric={reasoning:1}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, you don't know the loss surface in advance (I've been able to plot it here because our model only has a single parameter and our dataset is very simple). In fact, when we get to neural networks, we'll be dealing with tens, hundreds, thousands, or even millions of parameters resulting in an incredibly complex loss surface that is impossible to visualize.\n",
    "\n",
    "Without knowing what the loss surface looks like you won't know whether you've optimized your model based on the global minimum or if you're stuck in a local minimum. What is one method you could use to try and avoid getting stuck in a local minima?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Your answer goes here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Logistic Regression from Scratch\n",
    "<hr>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, I'm going to get you to implement your very own Logistic Regression model from scratch, just like sklearn's `LogisticRegression` class.\n",
    "\n",
    "To train and test our model, we're going to try predicting the presence of [Pulsar Stars](https://en.wikipedia.org/wiki/Pulsar), using a dataset available on the UCI Machine Learning Repository [here](https://archive.ics.uci.edu/ml/datasets/HTRU2). Understanding the dataset is not that important to the question but I'll briefly explain it below.\n",
    "\n",
    "Pulsar stars are a rare type of (dead) star that produce regular pulses of radiation in two symmetrical beams, like a lighthouse, as shown in the gif below. They can rotate up to hundreds of times per second.\n",
    "\n",
    "<img src=\"https://www.ligo.org/science/Publication-S6VSR24KnownPulsar/Images/lightnew.gif\" width=\"400\">\n",
    "\n",
    "\n",
    "Source: [www.ligo.org](https://www.ligo.org/science/Publication-S6VSR24KnownPulsar/Images/lightnew.gif)\n",
    "\n",
    "Pulsars are hard to find, yet they are extremely useful for a variety of astrophysical purposes. As pulsars spin, their radiation beams sweep across the sky and can be detected from Earth (if they cross our line of sight), producing a detectable pattern of radiation in our measurement devices. Scientists typically average this signal over many rotations of the pulsar and then describe the pattern using various statical measures like the mean, standard deviation, skewness, etc. These are the features we have access to in this exercise.\n",
    "\n",
    "We have a training set of 4 features and 9273 observations, and we wish to classify which observations are pulsar stars (i.e., we have a simple binary classification problem). You can read more about pulsar stars and this dataset [here](https://archive.ics.uci.edu/ml/datasets/HTRU2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below reads in the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/pulsar_data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1\n",
    "rubric={accuracy:1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, use sklearn's `train_test_split()` to split data into training and testing sets using the `test_size` and `random_state` given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.2\n",
    "random_state = 1132\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2\n",
    "rubric={accuracy:2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, standardize the feature data in the training and test sets using sklearn's `StandardScaler()`. As we saw in Lecture 2, standardizing often helps with optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOU MUST USE THE SAME SCALER TO FIT AND TRANSFORM!!!\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3\n",
    "rubric={accuracy:6}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, time to build our very own logistic regression class. The cell below defines the logistic loss function and its gradient which we saw previously in Lecture 2 (you can also find their derivations in Appendix B)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(w, X):\n",
    "    \"\"\"Sigmoid function.\"\"\"\n",
    "    return 1 / (1 + np.exp(-X @ w))\n",
    "\n",
    "def logistic_loss(w, X, y):\n",
    "    \"\"\"Logistic loss.\"\"\"\n",
    "    return -(y * np.log(sigmoid(w, X)) + (1 - y) * np.log(1 - sigmoid(w, X))).mean()\n",
    "\n",
    "def logistic_loss_grad(w, X, y):\n",
    "    \"\"\"Gradient of logistic loss.\"\"\"\n",
    "    return (X.T @ (sigmoid(w, X) - y) / len(X))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, complete the code below to create the class `MyLogisticRegression`. This is our custom logistic regression class with its very own `fit`, `predict`, `predict_proba`, and `score` methods. Here is what you should do:\n",
    "\n",
    "1. Use `scipy.optimize.minimize` in the `fit` method to solve the optimization problem (we did this in Lecture 2). I suggest initializing your weights to all zeros in the `fit` method before calling `minimize()` and passing in the gradient (`logistic_loss_grad`) to `minimize()` using the `jac` argument, just like we did in Lecture 2.\n",
    "\n",
    "2. `predict_proba` should just return the predicted probabilities from your model (i.e., the output of the `sigmoid()` function).\n",
    "3. `predict` should convert predicted probabilities to 0's and 1's depending on the `threshold`.\n",
    "4. `score` should return the accuracy of the model, i.e., the accuracy when comparing the output of `predict` to the true class values `y`.\n",
    "5. I've provided some `assert` statements to help you see if you're on the right track."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLogisticRegression:\n",
    "\n",
    "    def __init__(self, method=\"L-BFGS-B\"):\n",
    "        self.method = method  # optimization method to use in scipy.optimize.minimize\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit the model according to the given training data.\"\"\"\n",
    "        ...\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Probability estimates for samples in X.\"\"\"\n",
    "        ...\n",
    "    \n",
    "    def predict(self, X, threshold=0.5):\n",
    "        \"\"\"Predict class labels (0 or 1) for samples in X.\"\"\"\n",
    "        ...\n",
    "    \n",
    "    def score(self, X, y, threshold=0.5):\n",
    "        \"\"\"Accuracy of model on the given test data and labels.\"\"\"\n",
    "        ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyLogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "assert model.w.size == 4, \"Wrong number of parameters\"\n",
    "assert (model.predict_proba(X_test) <= 1).all(), \"Predicted probabilities exceed 1\"\n",
    "assert set(model.predict(X_test)) == {0, 1}, \"Model predicts value other than 0 and 1\"\n",
    "assert np.isclose(model.score(X_train, y_train), 0.93, atol=0.05), \"Model score on training data seems to be off...\"\n",
    "assert np.isclose(model.score(X_test, y_test), 0.92, atol=0.05), \"Model score on test data seems to be off...\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you get a `divide by zero encountered in log` when running the code above and fitting your model?\n",
    "\n",
    "That's great! This is caused by underflow, overflow and floating point rounding errors. If you're interested, read on, if you're not, ignore this.\n",
    "\n",
    "The error occurs when we try to take the log of the sigmoid function in the `logistic_loss()` function: `np.log(sigmoid(w, X))` or `np.log(1 - sigmoid(w, X))`. The term `sigmoid(w, X)` may be 0 or 1 if `np.exp(-X @ w)` in the `sigmoid()` function overflows or underflows respectively, and that means we'd be trying to take the log of 0 which gives us the error. In exercises 5 and 6, you'll get introduced to clever ways of avoiding this error. The tricks you'll implement are used in real Python computing packages.\n",
    "\n",
    "In fact, if you replace the `logistic_loss()` function defined earlier with the sklearn implementation:\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import log_loss\n",
    "def logistic_loss(w, X, y):\n",
    "    y_hat = sigmoid(w, X)\n",
    "    return log_loss(y, y_hat)\n",
    "```\n",
    "\n",
    "you'll avoid this error because sklearn uses the tricks introduced in exercises 5 and 6 to avoid underflow/overflow errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4\n",
    "rubric={accuracy:3}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a piece of code that reports the training accuracy and test accuracy of your model along with a confusion matrix for the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5\n",
    "rubric={accuracy:2, reasoning:2}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the fitting speed, fitted coefficients and test accuracy of your logistic regression implementation called `MyLogisticRegression` to the sklearn one (`from sklearn.linear_model import LogisticRegression`). For a fair comparison, use the following hyperparameters with scikit-learn's `LogisticRegression`:\n",
    "\n",
    "- `C=1e8` to (mostly) disable regularization for sklearn, since your implementation doesn't use regularization.\n",
    "\n",
    "- `fit_intercept=False` since your code above doesn't fit an intercept term.\n",
    "\n",
    "Is there a significant difference in the fit speed, coefficients or test accuracy between the two models?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Your answer goes here, replacing this text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 (OPTIONAL)\n",
    "rubric={reasoning:1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, sklearn's `LogisticRegression` implements regularization which is typically a good idea. Modify our custom class `MyLogisticRegression` to include L2 regularization. You actually don't need to change the `MyLogisticRegression` class at all for this, we just need to change the loss function and gradient of the loss function!\n",
    "\n",
    "Here's the log loss function we're already familiar with:\n",
    "\n",
    "$$f(w)=-\\frac{1}{n}\\sum_{i=1}^ny_i\\log\\left(\\frac{1}{1 + \\exp(-w^Tx_i)}\\right) + (1 - y_i)\\log\\left(1 - \\frac{1}{1 + \\exp(-w^Tx_i)}\\right)$$\n",
    "\n",
    "And here it is with L2 regularization:\n",
    "\n",
    "$$f(w)=-\\frac{1}{n}\\sum_{i=1}^ny_i\\log\\left(\\frac{1}{1 + \\exp(-w^Tx_i)}\\right) + (1 - y_i)\\log\\left(1 - \\frac{1}{1 + \\exp(-w^Tx_i)}\\right) + \\frac{\\lambda}{2n}\\sum_{j=1}^dw_j^2$$\n",
    "\n",
    "Where $d$ is the number of parameters in the model (4 in this case, one for each feature), and $\\lambda$ is the regularization parameter.\n",
    "\n",
    "Re-define the functions `logistic_loss()` and `logistic_loss_grad()` to include L2 regularization (you'll need to work out the formula for the gradient of the loss yourself, it's not very difficult).\n",
    "\n",
    "Your model should then give similar results to sklearn's `LogisticRegression()` with the parameter `C=1`. I've included a cell for you to test this out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test that model coefficients are similar to sklearn\n",
    "my_model = MyLogisticRegression()\n",
    "my_model.fit(X_train, y_train)\n",
    "sk_model = LogisticRegression(C=1, fit_intercept=False)\n",
    "sk_model.fit(X_train, y_train)\n",
    "\n",
    "print(f\"MyLogisticRegression coefs: {my_model.w}\")\n",
    "print(f\"sklearn LogisticRegression coefs: {sk_model.coef_[0]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: `log(1 + exp(z))`\n",
    "<hr>\n",
    "\n",
    "rubric={accuracy:3}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Lecture 1 we discussed computing $\\log(1+\\exp(z))$ and we wrote a better version of the function for when $z\\gg1$ which avoids overflow errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_1_plus_exp(z):\n",
    "    return np.log(1 + np.exp(z))\n",
    "\n",
    "\n",
    "def log_1_plus_exp_safe(z):\n",
    "    if z > 100:\n",
    "        return z\n",
    "    else:\n",
    "        return np.log(1 + np.exp(z))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might have thought this was not all that important at the time, but as we just saw in Exercise 4.3, even fairly simple equations like the log-loss function are at the mercy of underflow and overflow errors. Now let's consider the case of $z\\ll -1$, i.e., when $z$ is a large negative number. In that case, we get zero with both of the above implementations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(log_1_plus_exp(-100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(log_1_plus_exp_safe(-100))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your tasks:\n",
    "\n",
    "1- Investigate why this is happening. Is the problem that $\\exp(-100)$ itself underflows?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2- Write a function `log_1_plus_exp_safer` that works well when $z\\gg 1$ and $z\\ll -1$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3- (OPTIONAL): Evaluating $\\log(1+\\exp(z))$ fails for $z$ greater than $z \\approx 709$, due to overflow in computing $\\exp(z)$. This means we can avoid this error by using the approximation $\\log(1+\\exp(z)) \\approx z$ with any $z_{\\text{threshold}}$ smaller than $\\approx 709$. In the lecture, we arbitrarily set $z_{\\text{threshold}} = 100$.\n",
    "\n",
    "Now question for you: Can we reduce the inaccuracy due to approximation errors by setting $z_{\\text{threshold}}$ to values smaller than $100$? Does it make sense to put a lower bound on $z_{\\text{threshold}}$? Explain your reasoning."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Your answer goes here, replacing this text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## (OPTIONAL) Exercise 6: Softmax logistic regression and log-sum-exp\n",
    "\n",
    "<hr>\n",
    "\n",
    "rubric={reasoning:1}\n",
    "\n",
    "In the \"multinomial\" (aka softmax) approach to multi-class logistic regression, your loss ends up having one term for each class, so you get something of the form $\\log \\sum_{k=1}^c \\exp(z_k)$, where $c$ is the number of classes, and $z_k$ is a quantity involving the weights and training data.\n",
    "\n",
    "For any choice of a constant $a$, we can rewrite this as follows:\n",
    "\n",
    "$$\\begin{align}\\log \\displaystyle\\sum_{k=1}^c \\exp(z_k) &= \\log \\displaystyle\\sum_{k=1}^c \\exp(z_k-a+a)\\\\ &= \\log \\left( \\displaystyle\\sum_{k=1}^c \\exp(z_k-a)\\exp(a) \\right) \\\\ &= \\log \\left( \\exp(a)\\displaystyle\\sum_{k=1}^c \\exp(z_k-a)\\right) \\\\ &= \\log \\left( \\exp(a) \\right) + \\log\\displaystyle\\sum_{k=1}^c \\exp(z_k-a) \\\\ &= a+ \\log \\displaystyle\\sum_{k=1}^c \\exp(z_k-a)\\end{align}$$\n",
    "\n",
    "Note: you only need to look at the first and last expression - the middle parts are only there to convince you they are indeed equivalent.\n",
    "\n",
    "1. Explain why this final expression might be more numerically stable and why $a=\\max \\{z_1,z_2,\\ldots,z_c\\}$ is a sensible choice.\n",
    "\n",
    "2. If $a=\\max \\{z_1,z_2,\\ldots,z_c\\}$, this trick seems to rely on the fact that overflow is more of a danger than underflow, because we may now compute $\\exp()$ of some very large negative values if $a$ is large. Explain why overflow is more of a problem than underflow in this calculation.\n",
    "\n",
    "3. Write a python function `log_sum_exp` that takes in an array `z` and computes the _original_ expression. Write another function `log_sum_exp_stable` that computes the final (more stable) expression using $a=\\max \\{z_1,z_2,\\ldots,z_c\\}$. Discuss the behaviour of the two functions. Also, compare your implementation to In fact, [`scipy.special.logsumexp`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.logsumexp.html#scipy.special.logsumexp) for one or two cases."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Your answer goes here, replacing this text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
