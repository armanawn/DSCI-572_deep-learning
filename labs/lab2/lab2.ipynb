{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/dsci572_header.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2: Stochastic Gradient Descent & Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "<hr>\n",
    "\n",
    "rubric={mechanics:3}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4aecf0223bc592cc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "- Follow the [general lab instructions](https://ubc-mds.github.io/resources_pages/general_lab_instructions/)\n",
    "- Upload a PDF version of your lab notebook to Gradescope, in addition to the .ipynb file.\n",
    "- Add a link to your GitHub repository here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Stochastic Gradient Descent\n",
    "<hr>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a Python function that performs gradient descent, it's much like the code you saw in Lectures and in Lab 1. I just made a few small changes:\n",
    "\n",
    "- Removed the docstring and `print` statements for brevity.\n",
    "\n",
    "- The algorithm stops after `n_iters`.\n",
    "\n",
    "I also include functions to calculate the MSE and gradient of MSE for a linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(f, f_grad, w0, X, y, n_iters=1000, α=0.001):\n",
    "    w = w0\n",
    "    for _ in range(n_iters):\n",
    "        w = w - α * f_grad(w, X, y)\n",
    "    return w\n",
    "\n",
    "\n",
    "def mse(w, X, y):\n",
    "    return np.mean((X @ w - y) ** 2)\n",
    "\n",
    "\n",
    "def mse_grad(w, X, y):\n",
    "    return X.T @ (X @ w) - X.T @ y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we do ordinary least squares linear regression on the Boston house-prices dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
    "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
    "X = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
    "y = raw_df.values[1::2, 2]\n",
    "\n",
    "X = MinMaxScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "w0 = np.zeros(X.shape[1])\n",
    "w_gd = gradient_descent(mse, mse_grad, w0, X, y, n_iters=10**4)\n",
    "\n",
    "print(f\"Fitting time = {time.time() - start:.4f}s\")\n",
    "print(\"Weights:\")\n",
    "w_gd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now compare to sklearn's `LinearRegression()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "lr = LinearRegression(fit_intercept=False).fit(X, y)\n",
    "print(f\"Fitting time = {time.time() - start:.4f}s\")\n",
    "print(\"Weights:\")\n",
    "lr.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the coefficients obtained from gradient descent are very similar to those obtained by sklearn's `LinearRegression()` (although sklearn is much faster)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1\n",
    "rubric={accuracy:5}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise your task is to implement a function `stochastic_gradient_descent`, that performs SGD **using the `gradient_descent` function provided above**. You can have your function accept the same arguments as the `gradient_descent` function above, except:\n",
    "\n",
    "- Change `n_iters` to `n_epochs`\n",
    "\n",
    "- Add an extra `batch_size` argument\n",
    "\n",
    "- Your implementation of SGD should follow **Approach 1** from Lecture 3: for each epoch, shuffle the dataset and then divide it into batches. If the number of samples is not divisible by the `batch_size`, your last batch will have less than `batch_size` samples. You can either throw this last batch away or use it (I usually choose to use it and that's the default in PyTorch).\n",
    "\n",
    "- You can leave `α` constant for all iterations. \n",
    "\n",
    "The pedagogical goal here is to help you see how SGD relates to regular \"vanilla\" gradient descent. In reality it would be fine to implement SGD \"from scratch\" without calling a GD function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent(f, f_grad, w0, X, y, n_epochs=1, α=0.001, batch_size=1):\n",
    "    \"\"\"DON'T FORGET TO WRITE DOCSTRINGS!\"\"\"\n",
    "    ...\n",
    "\n",
    "\n",
    "stochastic_gradient_descent(mse, mse_grad, w0, X, y)  # Test your function with defaults"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2\n",
    "rubric={accuracy:1}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show that when the batch size is set to the whole training set (i.e., `batch_size=len(X)`), you get exactly the same estimated coefficients with SGD and GD. Use the same learning rate (`α=0.001`) and number of epochs (`10**4`) for both algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: `LogisticRegression` vs `SGDClassifier`\n",
    "<hr>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we'll explore training a classifier with SGD on the [Sentiment140 dataset](http://help.sentiment140.com/home), which contains tweets labeled with sentiment associated with a brand, product, or topic. Please start by doing the following:\n",
    "\n",
    "1. Download the dataset from [here](http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip)\n",
    "\n",
    "2. Unzip\n",
    "3. Copy the file `training.1600000.processed.noemoticon.csv` into the current directory\n",
    "4. Create a `.gitignore` and add this dataset to it so you don't accidentally commit the dataset.\n",
    "\n",
    "Once you're done the above, steps, run the starter code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading and preprocessing\n",
    "df = pd.read_csv(\n",
    "    \"training.1600000.processed.noemoticon.csv\",\n",
    "    encoding=\"ISO-8859-1\",\n",
    "    names=[\"label\", \"id\", \"date\", \"no_query\", \"name\", \"text\"],\n",
    ")\n",
    "df[\"label\"] = df[\"label\"].map({0: \"neg\", 4: \"pos\"})  # change 0's to \"neg\" and 4's to \"pos\"\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we split the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df[\"text\"], df[\"label\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=2021)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we encode it using `CountVectorizer`, which may take a while:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer(stop_words='english')\n",
    "X_train = vec.fit_transform(X_train)\n",
    "X_test = vec.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that our training data is rather large compared to datasets we've explored in the past:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily, `CountVectorizer()` returns us a sparse matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that a sparse matrix is a more efficient representation of a matrix that contains many 0's. What percentage of our array is non-zero?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{X_train.nnz / np.prod(X_train.shape) * 100:.5f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So few non-zero values. Now let's train a classifier. Please note that this may take a while. Also, you'll see a `ConvergenceWarning`, which is not important and you can ignore it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time.time()\n",
    "lr.fit(X_train, y_train)\n",
    "print(f\"Training took {time.time() - t:.1f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Train score: {lr.score(X_train, y_train):.2f}\")\n",
    "print(f\"Test score: {lr.score(X_test, y_test):.2f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1\n",
    "rubric={accuracy:2}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In sklearn, there is a classifier called `linear_model.SGDClassifier`. See the docs [here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html) (there's also a `SGDRegressor` but we won't look at that here). As the name suggests, this model can train linear classifiers with SGD in the true sense of the algorithm. That is, 1 sample per iteration (i.e., batch size of 1).\n",
    "\n",
    "Train a logistic regression model on the same dataset above using `SGDClassifier`. Compare the training time of your `SGDClassifier` to `LogisticRegression`. You'll need to specify the correct `loss` argument in `SGDClassifier()` to train a logistic regression model. [Read the docstring](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html) to find the appropriate `loss`.\n",
    "\n",
    "The pedagogical goal here is to demonstrate how using just one sample per iteration in SGD can significantly speed up training, and accuracy-wise, is not a terrible idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2\n",
    "rubric={reasoning:2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discuss the **training** and **test** accuracies of your `SGDClassifier()` and `LogisticRegression()` models. Is there any difference between the two models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3\n",
    "rubric={reasoning:3}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`SGDClassifier` and `LogisticRegression` have an `n_iter_` attribute which you can check after fitting (in these sklearn models, `n_iter_` is equivalent to number of epochs).\n",
    "\n",
    "1. Based on these numbers, what is one possible explanation for `SGDClassifier`'s faster convergence compared to `LogisticRegression`? Explain your reasoning.\n",
    "\n",
    "2. Using the `max_iter` parameter, do a **fair** experiment where `SGDClassifier` and `LogisticRegression` do the same number of passes through the dataset, and comment on the results.\n",
    "\n",
    "**Note:** To be completely fair we should also make sure that the tolerance and regularization strength in both models is the same (by default they are not). But you don't need to worry about that here. Just focus on the number of iterations/epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Neural Networks by Hand\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1\n",
    "rubric={accuracy:4}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural networks we've seen in Lecture 4 are layers of functions where each layer is made up of the previous layer's output, multiplied by some weights, with some biases added, and passed through an activation function (we call these networks _fully-connected feed-forward networks_).\n",
    "\n",
    "Mathematically, the output of each layer $l$ of the network has the following form:\n",
    "\n",
    "$$ x^{(l)} = h\\left( W^{(l)} x^{(l-1)} + b^{(l)}\\right) $$\n",
    "\n",
    "where:\n",
    "- $h()$ is the activation function\n",
    "\n",
    "- $W^{(l)}$ is a matrix of weights\n",
    "- $b^{(l)}$ is a vector of biases \n",
    "- $x^{(l)}$ is the output of layer $l$:\n",
    "    - $x^{(0)}$ are the inputs\n",
    "    - $x^{(1)}$ are the outputs of the first hidden layers, i.e., $x^{(1)} = h\\left( W^{(1)} x^{(0)} + b^{(1)}\\right)$\n",
    "    - etc.\n",
    "    - $\\hat{y} = x^{(L)} = W^{(L)} x^{(L-1)} + b^{(L)}$, with $l=L$ indicating the output layer)\n",
    "\n",
    "Suppose that we use a neural network with one hidden layer with a **ReLU activation**. After training, we obtain the following parameters:\n",
    "\n",
    "$$\\begin{align}W^{(1)} &= \\begin{bmatrix}-2 & 2 & -1\\\\-1 & -2 & 0\\end{bmatrix},  &b^{(1)}&=\\begin{bmatrix}2 \\\\ 0\\end{bmatrix} \\\\ W^{(2)} &= \\begin{bmatrix}3 & 1\\end{bmatrix},  &b^{(2)}&=-10\\end{align}$$\n",
    "\n",
    "For a training example with features $x_0 = \\begin{bmatrix}3 \\\\-2 \\\\ 2\\end{bmatrix}$, what are the values in this network of $x^{(1)}$ and $\\hat{y}$? Show your work using code cells or LaTeX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2\n",
    "rubric={reasoning:4}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw this neural network using a circle/arrow diagram (similar to the ones provided in Lecture 4). Label the diagram with the weight/bias values given above. If you want to draw this diagram by hand, that is fine: you can take a photo of the drawing and put it in here. If you are doing so, make sure you upload the image to your repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "_Your drawing goes here._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Predicting Fashion\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Exercise, you're going to train a neural network using the [Fashion-MNIST dataset](https://github.com/zalandoresearch/fashion-mnist). Fashion-MNIST is a set of 28 x 28 pixel greyscale images of clothes.\n",
    "\n",
    "Some of you may have worked with this dataset before, it's a classic one. This dataset is ideal for your first PyTorch exercise. Below is a sample of some of the images in the dataset. We have 10 classes in the dataset: T-shirt/tops, Trousers, Pullovers, Dresses, Coats, Sandals, Shirts, Sneakers, Bags, and Ankle Boots.\n",
    "\n",
    "<img src=\"img/fashion-mnist.png\" width=\"500\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this exercise is to develop a network that can correctly predict a given image of \"fashion\" into one of the 10 classes. This is a multi-class classification task, our model should spit out 10 probabilities for a given image, one probability for each class. Ideally, the class our model predicts with maximum probability is the correct one.\n",
    "\n",
    "The below cell will download and load in the Fashion-MNIST data for you. We'll talk more about this process later, but briefly:\n",
    "\n",
    "- Think of images as `ndarrays` of data, in the case of grayscale images like we have here, each pixel has a value between 0 and 1 indicating how \"bright\" that pixel is. So each image here is just a 28 x 28 `ndarray` with values ranging from 0 to 1 (when we get to colour images, it's exactly the same, except each pixel has 3 values, one for each of the colour channels Red, Blue, Green. If we had colour images here our array would be 28 x 28 x 3).\n",
    "\n",
    "- `transform`: applies some transformations to the images. Here we are converting the data to tensors. We'll work with these more next week so don't worry too much about them.\n",
    "\n",
    "- `torch.utils.data.DataLoader`: these are _data loaders_. Think of them as generators. During training/testing, we can easily query them for a batch of data of size `BATCH_SIZE`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell might give you a warning, but no need to worry about that (if you run the cell again it disappears):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "# Define a transform to normalize the data, which usually helps with training\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# Download data\n",
    "trainset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=True, transform=transform)\n",
    "testset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=False, transform=transform)\n",
    "\n",
    "# Create data loaders (these are just generators that will give us `batch_size` samples as a time)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Class labels\n",
    "class_labels = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "                'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot a random image (run this cell as many times as you like to see different images):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = next(iter(trainloader))  # Get a random batch of 64 images\n",
    "i = np.random.randint(0, 64)            # Choose one image at random\n",
    "plt.imshow(image[i, 0], cmap='gray')    # Plot\n",
    "plt.title(class_labels[label[i]]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1\n",
    "rubric={accuracy:3}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice in the plot above that our image is 28 x 28 pixels. How do we feed this into a neural network? Well we can flatten it out into a vector of 784 elements (28 x 28 = 784) and create 784 input nodes. We'll do this later on. For now, all I want you to do is create a new class for our PyTorch neural network model defining a classifier with the following architecture:\n",
    "\n",
    "- hidden layer that goes from `input_size` -> 256 nodes\n",
    "\n",
    "- ReLU activation function\n",
    "- hidden layer that goes from 256 nodes -> 128 nodes\n",
    "- ReLU activation function\n",
    "- hidden layer that goes from 128 nodes -> 64 nodes\n",
    "- ReLU activation function\n",
    "- output layer that goes from 64 nodes -> `output_size` nodes\n",
    "\n",
    "I've given you some starter code to get you going.\n",
    "\n",
    "**Note:** When we create our model in a later exercise we will specify `input_size=784` and `output_size=10`. The `784` is the flattened 28 x 28 image and the output size of 10 is so that we have one node for each item of clothing (remember we have 10 classes), and each node will contain the probability of that item of clothing being the one in a particular input image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        self.main = ...\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.main(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2\n",
    "rubric={accuracy:1}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your model has `input_size=784` and `output_size=10`, how many parameters does your model have? Write your manual calculations, and verify your result using `torchsummary.summary()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3\n",
    "rubric={accuracy:3}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We haven't trained yet, but let's test out your network. The below function will help you plot your network's predictions for a particular image using `matplotlib`, run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prediction(image, label, predictions):\n",
    "    \"\"\"Plot network predictions with matplotlib.\"\"\"\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(figsize=(8, 4), ncols=2)  # Plot\n",
    "    ax1.imshow(image[0], cmap='gray')\n",
    "    ax1.axis('off')   \n",
    "    ax1.set_title(class_labels[label])\n",
    "    ax2.barh(np.arange(10), predictions.data.numpy().squeeze())   \n",
    "    ax2.set_title(\"Predictions\")\n",
    "    ax2.set_yticks(np.arange(10))\n",
    "    ax2.set_yticklabels(class_labels)\n",
    "    ax2.set_xlim(0, 1)\n",
    "    plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Classifier(input_size=784, output_size=10)\n",
    "\n",
    "# Test on training images (run as many times as you like!)\n",
    "image, label = next(iter(trainloader))        # Get a random batch of 64 images\n",
    "predictions = model(image[0].reshape(1, -1))  # Get first image, flatten to shape (1, 784) and predict it\n",
    "predictions = nn.Softmax(dim=1)(predictions)  # Coerce predictions to probabilities using Softmax()\n",
    "plot_prediction(image[0], label[0], predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, those predictions are probably pretty bad, because the model is not trained yet.\n",
    "\n",
    "Below is a training function similar to what we saw in Lecture 4. The only difference is that when I'm creating `y_hat` (model predictions), I'm reshaping my `X` data from `(batch_size, 1, 28, 28)` to `(batch_size, 784)`, so that we can feed it into our network. `X.shape[0]` is the batch size, and the `-1` just says _flatten remaining dimensions into a single dimension_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(model, criterion, optimizer, dataloader, epochs=5, verbose=True):\n",
    "    \"\"\"Simple training wrapper for PyTorch network.\"\"\"\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        losses = 0\n",
    "        for X, y in dataloader:\n",
    "            optimizer.zero_grad()       # Clear gradients w.r.t. parameters\n",
    "            y_hat = model(X.reshape(X.shape[0], -1))  # Reshape data to (batch_size, 784) and forward pass to get output\n",
    "            loss = criterion(y_hat, y)  # Calculate loss\n",
    "            loss.backward()             # Getting gradients w.r.t. parameters\n",
    "            optimizer.step()            # Update parameters\n",
    "            losses += loss.item()       # Add loss for this batch to running total\n",
    "            \n",
    "        if verbose:\n",
    "            print(f\"epoch: {epoch + 1}, loss: {losses / len(dataloader):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now define an appropriate `criterion` and `optimizer` to train your model with.\n",
    "\n",
    "- We are doing multi-class classification here, what loss function do we use for this case?\n",
    "- Use any optimizer you like, but I recommend Adam (by the way, optional Exercise 5 of this lab is about implementing Adam from scratch)\n",
    "\n",
    "I already created the dataloader `trainloader` for you at the start of this exercise. Pass all these things to `trainer()` to train your model. Remember that it may take a few minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4\n",
    "rubric={accuracy:1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test out your newly trained network on the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model on training images (run as many times as you like!)\n",
    "image, label = next(iter(trainloader))        # Get a random batch of 64 images\n",
    "\n",
    "predictions = model(image[0].view(1, -1))     # Get first image, flatten to shape (1, 784) and predict it\n",
    "predictions = nn.Softmax(dim=1)(predictions)  # Coerce predictions to probabilities using Softmax()\n",
    "plot_prediction(image[0], label[0], predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And test it out on the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model on testing images (run as many times as you like!)\n",
    "image, label = next(iter(testloader))        # Get a random batch of 64 images\n",
    "predictions = model(image[0].view(1, -1))     # Get first image, flatten to shape (1, 784) and predict it\n",
    "predictions = nn.Softmax(dim=1)(predictions)  # Coerce predictions to probabilities using Softmax()\n",
    "plot_prediction(image[0], label[0], predictions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully your predictions look good. That's all there is to it. You just created your first neural network classifier!\n",
    "\n",
    "**Now comes the question for you:** In this exercise we used a `BATCH_SIZE=64`. Describe how choosing a small vs. large batch size affects the convergence of the optimization algorithm, as well as hardware requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Models Are Only as Good as Their Underlying Assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our network was trained on clothing images, but that doesn't mean we can't try it on other images.\n",
    "\n",
    "Let's see what our model thinks of me:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = torch.from_numpy(plt.imread(\"img/arman.png\"))\n",
    "predictions = model(image.view(1, -1))        # Flatten image to shape (1, 784) and predict it\n",
    "predictions = nn.Softmax(dim=1)(predictions)  # Coerce predictions to probabilities using Softmax()\n",
    "label = predictions.argmax(dim=1)             # Get class label from max probability\n",
    "plot_prediction(image.numpy()[None, :, :], label, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, looks like everything is a joke to ML models, except for their training data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (OPTIONAL) Exercise 5: Implementing Adam Optimizer From Scratch\n",
    "<hr>\n",
    "\n",
    "rubric={accuracy}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adam (adaptive moment estimation) is an optimization algorithm that we'll be using a lot for the rest of the course. [Here](https://arxiv.org/abs/1412.6980)'s the original paper that proposed the algorithm. It is essentially a fancier version of SGD. Without getting too technical, Adam really adds two additional features to SGD:\n",
    "\n",
    "1. Momentum: which uses past gradients to help improve convergence speed, reduce noise in the path to the minimum, and avoid local minima.\n",
    "\n",
    "2. Per-parameter learning rate: a learning rate is maintained and adapted for each parameter as iterations of optimization proceed.\n",
    "\n",
    "I recommend [reading this article](https://ruder.io/optimizing-gradient-descent/index.html) to learn more, but Adam boils down to the following weight updating procedure:\n",
    "\n",
    "$$\\mathbf{w}_{t+1} = \\mathbf{w}_{t} - \\frac{\\alpha}{\\sqrt{\\hat{v}_{t}} + \\epsilon} \\hat{m}_{t}$$\n",
    "\n",
    "The various components required for that equation:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\hat{m}_{t} &= \\frac{m_{t}}{1 - \\beta_{1}^{t}}\\\\\n",
    "\\hat{v}_{t} &= \\frac{v_{t}}{1 - \\beta_{2}^{t}}\n",
    "\\end{align}$$\n",
    "\n",
    "$$\\begin{align}\n",
    "m_{t} &= \\beta_{1} m_{t-1} + (1 - \\beta_{1}) g_{t}\\\\\n",
    "v_{t} &= \\beta_{2} v_{t-1} + (1 - \\beta_{2}) g_{t}^{2}\n",
    "\\end{align}$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $t$ is the iteration of optimization, it increments up by one each time you update $\\mathbf{w}$. Note that in the equation for $\\hat{m}_{t}$ and $\\hat{v}_{t}$, $\\beta_{1}$ and $\\beta_{2}$ are raised to the power of $t$.\n",
    "\n",
    "- $g_{t}$ is the gradient of the loss function w.r.t to the parameters $w$.\n",
    "- $m_{t}$ is known as the first moment (the mean) of the gradients. Initialize as 0.\n",
    "- $v_{t}$ is known as the second moment (the uncentered variance) of the gradients. Initialize as 0.\n",
    "- $\\alpha$ is the learning rate. 0.1 is a good start.\n",
    "- $\\epsilon$ is just a term to prevent division by zero. Default: $10^{-8}$.\n",
    "- $\\beta_{1}$ is a hyperparameter that controls the influence of past gradients on subsequent updates. Default: $0.9$.\n",
    "- $\\beta_{2}$ is a hyperparameter that controls the influence of past gradients on subsequent updates. Default: $0.999$.\n",
    "\n",
    "Here's a function with both local and global minima. We know in advance that the global minimum occurs at $w_{opt}=4$. I want you to find this value using Adam optimization and starting at $w \\neq w_{opt}$. I've provided you the function (`f()`), the MSE loss w.r.t this function (`loss()`), and the gradient of the loss (`loss_grad()`). Run the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(w, X):\n",
    "    \"\"\"Squiggly function\"\"\"\n",
    "    return w * np.cos(w * X)\n",
    "\n",
    "def loss(w, X, y):\n",
    "    \"\"\"MSE loss.\"\"\"\n",
    "    return np.mean((f(w, X) - y) ** 2)\n",
    "\n",
    "def loss_grad(w, X, y):\n",
    "    \"\"\"Gradient of MSE.\"\"\"\n",
    "    t = np.cos(w * X) - w * X * np.sin(w * X)\n",
    "    return np.mean((f(w, X) - y) * t)\n",
    "\n",
    "w_opt = 4\n",
    "X = np.arange(-3, 3, 0.1)\n",
    "y = f(w_opt, X)\n",
    "l = [loss(w, X, y) for w in np.arange(-10, 11, 0.1)]\n",
    "\n",
    "plt.plot(np.arange(-10, 11, 0.1), l)\n",
    "plt.xlabel(\"w\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.grid(True);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task here is to implement Adam from scratch. Then use it to find $w_{opt}$ for the above function. I've provided some code below that you should run when you're ready. Note that:\n",
    "\n",
    "- I've specified a default of 100 epochs. We have a *tiny* dataset here of 60 samples so this is nothing substantial. Feel free to add more epochs if you wish.\n",
    "\n",
    "- You can start with the default values for the various Adam terms I give in the equations above.\n",
    "- You *may* need to play around with the hyperparameter $\\alpha$ to get to the minimum (I've given a default of 0.3 in the starter code below. I didn't need to change this value in my solution). You can leave $\\beta_{1}$, $\\beta_{2}$ as is - Often we don't tune those ones and I didn't in my solution, but you can tune them if you want.\n",
    "- Adam uses batches just like SGD, so my solution has the ability to accept a `batch_size` argument. But you don't have to implement this functionality and I didn't include that argument in the starter code below. So feel free to just use all the data each iteration for simplicity like vanilla GD would do. If you're feeling adventurous, no one should stop you from throwing in a `batch_size` argument.\n",
    "\n",
    "The pedagogical goal here is to get you to implement Adam and play around with it to see how it can **jump over** local minima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Adam(X, y, w0, loss, loss_grad, n_epochs=100, alpha=0.3, beta1=0.9, beta2=0.999, eta=10e-8):\n",
    "    ...\n",
    "\n",
    "w0 = np.array([9])\n",
    "w = Adam(X, y, w0, loss, loss_grad)\n",
    "print(w)  # Should be close to 4"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "dsci572",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 | packaged by conda-forge | (main, Nov 22 2022, 08:25:29) [Clang 14.0.6 ]"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "f821000d0c0da66e5bcde88c37d59c8e0de03b40667fb62009a8148ca49465a0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
