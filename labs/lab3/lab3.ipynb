{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/dsci572_header.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "<hr>\n",
    "\n",
    "rubric={mechanics:3}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4aecf0223bc592cc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "- Follow the [general lab instructions](https://ubc-mds.github.io/resources_pages/general_lab_instructions/)\n",
    "\n",
    "- Upload a PDF version of your lab notebook to Gradescope, in addition to the .ipynb file.\n",
    "\n",
    "- Add a link to your GitHub repository here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, transforms, utils\n",
    "from torchsummary import summary\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams.update({'axes.grid': False})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Filters and Convolutions\n",
    "<hr>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I'm a huge fan of cats (small or big), we're going to use the image of a Cheetah ([source](https://unsplash.com/photos/hksj-fvUVek)) for this exercise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = torch.from_numpy(plt.imread(\"img/cheetah.png\"))[:, :, 0]\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J6QQPeGXO2pc"
   },
   "source": [
    "For each of the filters given below, convolve the above image with the given filter/kernel and briefly discuss why the results look the way they do. You'll need to:\n",
    "\n",
    "1. Created a `Conv2D` layer with PyTorch.\n",
    "\n",
    "2. Manually change the kernel weights. Weights in a `Conv2D` layer are 4D tensors (the 4 dimensions are: `[num_images=1, num_channels=1, kernel_rows, kernel_cols]`). I've given example code defining such a 4D tensor below. Functions that will help you create tensors: `torch.ones()`, `torch.zeros()`, `torch.full()`, etc. (we have much of the same functionality as we did in `NumPy`).\n",
    "3. Use the provided code to plot the original and convolved images.\n",
    "4. Explain the result in 1-2 sentences.\n",
    "\n",
    "I've provided an example below to get you started. You can assume the default `stride` and `padding` in the `Conv2D` layer.\n",
    "\n",
    "> The pedagogical goal here is to help you better understand what filters/kernels actually are and how they help us identify useful structure (like lines, curves, shapes, etc.) in images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_convolution(image, conv_layer):\n",
    "    \"\"\"\n",
    "    Convolve kernel over image and plot.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    image : torch.Tensor\n",
    "        Image to filter with kernel.\n",
    "    conv_layer : function\n",
    "        A PyTorch Conv2D layer to apply to image.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    matplotlib.image\n",
    "    \"\"\"\n",
    "\n",
    "    conv_image = conv_layer(image[None, None, :]).detach().squeeze()\n",
    "    fig, (ax1, ax2) = plt.subplots(figsize=(8, 4), ncols=2)\n",
    "    ax1.imshow(image, cmap='gray'); ax1.axis('off'); ax1.set_title(\"Original\")\n",
    "    ax2.imshow(conv_image, cmap='gray'); ax2.axis('off'); ax2.set_title(\"Filtered\")\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:**\n",
    "\n",
    "The kernel is a row vector of ten 0.1's, shape `(1, 10)`:\n",
    "\n",
    "$$\\text{kernel} = \\begin{bmatrix} 0.1 & 0.1 & 0.1 & 0.1 & 0.1 & 0.1 & 0.1 & 0.1 & 0.1 & 0.1 \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layer = torch.nn.Conv2d(1, 1, kernel_size=(1, 10))\n",
    "conv_layer.weight.detach_()\n",
    "kernel = torch.full((1, 1, 1, 10), 0.1)\n",
    "conv_layer.weight[:] = kernel\n",
    "plot_convolution(image, conv_layer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AJfVXMiCO2pp"
   },
   "source": [
    "**Example answer:**\n",
    "\n",
    "The filter is a horizontal bar of 0.1s. Therefore I would expect a blurring in the horizontal direction, meaning the _vertical_ edges get blurred (because these are the ones that change rapidly in the horizontal direction). This seems to be happening in the result. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1-z9WROaO2pr"
   },
   "source": [
    "### 1.1\n",
    "rubric={accuracy:2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kernel is a column vector of ten 0.1's, shape (10, 1):\n",
    "\n",
    "$$\\text{kernel} = \\begin{bmatrix} 0.1 \\\\ 0.1 \\\\ 0.1 \\\\ 0.1 \\\\ 0.1 \\\\ 0.1 \\\\ 0.1 \\\\ 0.1 \\\\ 0.1 \\\\ 0.1 \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LtPIw0w-O2pw"
   },
   "source": [
    "### 1.2\n",
    "rubric={accuracy:2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kernel is a matrix of 0's but with a 1 in the centre, shape (5, 5):\n",
    "\n",
    "$$\\text{kernel} = \\begin{bmatrix} 0 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 0 \\\\ \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "32tx26E6O2p1"
   },
   "source": [
    "### 1.3\n",
    "rubric={accuracy:2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kernel is a matrix of 0.01's, shape (10, 10):\n",
    "\n",
    "$$\\text{kernel} = \\begin{bmatrix} 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & \\\\ 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & \\\\ 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & \\\\ 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & \\\\ 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & \\\\ 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & \\\\ 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & \\\\ 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & \\\\ 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & \\\\ 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "32K6eYDKO2p8"
   },
   "source": [
    "### 1.4\n",
    "rubric={accuracy:2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kernel is a matrix of -0.125's, shape (3, 3):\n",
    "\n",
    "$$\\text{kernel} = \\begin{bmatrix} -0.125 & -0.125 & -0.125 \\\\ -0.125 & -0.125 & -0.125 \\\\ -0.125 & -0.125 & -0.125 \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: CNNs and Image Permutations\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gg208bROO2qN"
   },
   "source": [
    "Below is some code that trains a CNN on the classic [MNIST digits dataset](https://en.wikipedia.org/wiki/MNIST_database). This dataset contains 28 x 28 pixel images of hand written digits. Run through the code, it may take a few minutes to run the code, our training dataset has 60,000 samples and our validation dataset has 10,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "\n",
    "# Download data\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "trainset = datasets.MNIST('data/', download=True, train=True, transform=transform)\n",
    "validset = datasets.MNIST('data/', download=True, train=False, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "validloader = torch.utils.data.DataLoader(validset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Sample plot\n",
    "X, y = next(iter(trainloader))\n",
    "plt.imshow(X[0, 0, :, :], cmap=\"gray\")\n",
    "plt.title(f\"Number: {y[0].item()}\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, (5, 5)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2, 2)),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(2304, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 10)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.main(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(model, criterion, optimizer, trainloader, validloader, epochs=5, verbose=True):\n",
    "    \"\"\"Simple training wrapper for PyTorch network.\"\"\"\n",
    "    \n",
    "    train_loss, valid_loss, valid_accuracy = [], [], []\n",
    "    for epoch in range(epochs):  # for each epoch\n",
    "        train_batch_loss = 0\n",
    "        valid_batch_loss = 0\n",
    "        valid_batch_acc = 0\n",
    "        \n",
    "        # Training\n",
    "        for X, y in trainloader:\n",
    "            optimizer.zero_grad()       # Zero all the gradients w.r.t. parameters\n",
    "            y_hat = model(X)            # Forward pass to get output\n",
    "            loss = criterion(y_hat, y)  # Calculate loss based on output\n",
    "            loss.backward()             # Calculate gradients w.r.t. parameters\n",
    "            optimizer.step()            # Update parameters\n",
    "            train_batch_loss += loss.item()  # Add loss for this batch to running total\n",
    "        train_loss.append(train_batch_loss / len(trainloader))\n",
    "        \n",
    "        # Validation\n",
    "        with torch.no_grad():  # this stops pytorch doing computational graph stuff under-the-hood and saves memory and time\n",
    "            for X, y in validloader:\n",
    "                y_hat = model(X)\n",
    "                _, y_hat_labels = torch.softmax(y_hat, dim=1).topk(1, dim=1)\n",
    "                loss = criterion(y_hat, y)\n",
    "                valid_batch_loss += loss.item()\n",
    "                valid_batch_acc += (y_hat_labels.squeeze() == y).type(torch.float32).mean().item()\n",
    "        valid_loss.append(valid_batch_loss / len(validloader))\n",
    "        valid_accuracy.append(valid_batch_acc / len(validloader))  # accuracy\n",
    "        \n",
    "        # Print progress\n",
    "        if verbose:\n",
    "            print(f\"Epoch {epoch + 1}:\",\n",
    "                  f\"Train Loss: {train_loss[-1]:.3f}.\",\n",
    "                  f\"Valid Loss: {valid_loss[-1]:.3f}.\",\n",
    "                  f\"Valid Accuracy: {valid_accuracy[-1]:.2f}.\")\n",
    "    \n",
    "    results = {\"train_loss\": train_loss,\n",
    "               \"valid_loss\": valid_loss,\n",
    "               \"valid_accuracy\": valid_accuracy}\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MNIST_classifier()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "results = trainer(model, criterion, optimizer, trainloader, validloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You probably got pretty good accuracy there. Now, answer the questions below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1\n",
    "rubric={accuracy:1}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D4aYezc9O2qZ"
   },
   "source": [
    "How many parameters does the model have (you don't need to compute that by hand)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2\n",
    "rubric={reasoning:1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Lecture 6 I talk about how, when doing image classification with fully connected neural networks, the order of the pixels in the flattened image we feed into the network doesn't matter. In contrast, CNNs try to use the structure in the data to make predictions. Let's do an experiment and vertically flip all our MNIST training images like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.RandomVerticalFlip(p=1), transforms.ToTensor()])\n",
    "trainset = datasets.MNIST('data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Sample plot\n",
    "X, y = next(iter(trainloader))\n",
    "plt.imshow(X[0, 0, :, :], cmap=\"gray\")\n",
    "plt.title(f\"Number: {y[0].item()}\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've only flipped the training images, not the validation images. What do you think would happen to the validation scores if we train our network on these new flipped images?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3\n",
    "rubric={reasoning:2}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-train your network using the new `trainloader` of flipped images we defined above to test your answer to 2.2. Are the results what you expected? Can you justify the accuracy of your model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MNIST_classifier()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "results = trainer(model, criterion, optimizer, trainloader, validloader, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Bitmoji CNN\n",
    "<hr>\n",
    "\n",
    "rubric={accuracy:15}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the course repository is a folder (`lectures/data/bitmoji_rgb`) containing the Bitmoji dataset we saw in Lecture 5. Make sure you clone that repo and then copy the folder `bitmoji_rgb` to the `data` directory of this lab.\n",
    "\n",
    "We have 1228 images of both \"`arman`\" and \"`not_arman`\" for training (1714 images total), and 300 images of both \"`arman`\" and \"`not_arman`\" for validation (600 images total). We will resize the images to be 64 x 64 pixels.\n",
    "\n",
    "Your task here is to create and train a CNN on this data. You can create any architecture you wish, but you need to show that you have at least 90% accuracy on the validation dataset after training your CNN.\n",
    "\n",
    "I have prepared the training and validation loaders for you below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DIR = \"data/bitmoji_rgb/train/\"\n",
    "VALID_DIR = \"data/bitmoji_rgb/valid/\"\n",
    "\n",
    "IMAGE_SIZE = (64, 64)\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Load data and create dataloaders\n",
    "data_transforms = transforms.Compose([transforms.Resize(IMAGE_SIZE), transforms.ToTensor()])\n",
    "\n",
    "train_dataset = datasets.ImageFolder(root=TRAIN_DIR, transform=data_transforms)\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "valid_dataset = datasets.ImageFolder(root=VALID_DIR, transform=data_transforms)\n",
    "validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "print(train_dataset.class_to_idx)  # See which labels are assigned to each class\n",
    "\n",
    "# Plot samples (don't worry too much about this code)\n",
    "sample_batch = next(iter(trainloader))\n",
    "plt.figure(figsize=(10, 8)); plt.axis(\"off\"); plt.title(\"Sample Training Images\")\n",
    "plt.imshow(np.transpose(utils.make_grid(sample_batch[0], padding=1, normalize=True), (1,2,0)))\n",
    "print(sample_batch[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note that `datasets.ImageFolder()` assigns integer labels to your classes according to the alphabetic order of the data folders. For example, in this case label `0` is assigned to class `arman`, whereas label `1` is assigned to `not_arman`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's some code you can use to plot your model's predictions for a random image from the validation set. Run the following cell as many times as you like to see how well your CNN model works for different inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model.to('cpu')\n",
    "with torch.no_grad():\n",
    "    img = next(iter(trainloader))[0][0]\n",
    "    y_prob = torch.sigmoid(model(img[None, :, :, :]))\n",
    "    y_class = int(y_prob > 0.5)\n",
    "    print(f\"Prbability of being Arman: {1 - y_prob.item():g}\")\n",
    "    \n",
    "    plt.imshow(img.permute(1, 2, 0))\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"{['Arman', 'Not Arman'][y_class]}\", pad=10);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (CHALLENGING) Exercise 4: Neural Networks From Scratch\n",
    "<hr>\n",
    "\n",
    "rubric={accuracy:1% of total grade}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From scratch using only `NumPy` and `SciPy`, implement a one-hidden-layer neural network for classification using ReLUs and [scikit-learn's digit dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html).\n",
    "\n",
    "You're pretty much on your own for this question. It could be a real test of how good your coding skills are and your knowledge of how neural networks actually work. You'll need to code up the gradients and implement backpropagation yourself. **This is a lot of work for 1% of the grade; use your time wisely**. \n",
    "\n",
    "> Note: there are probably a lot of resources out there where people give their \"raw\" neural network implementations. If you're going to do this and you consult any sources, make sure you cite them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "dsci572",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "272px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "f821000d0c0da66e5bcde88c37d59c8e0de03b40667fb62009a8148ca49465a0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
