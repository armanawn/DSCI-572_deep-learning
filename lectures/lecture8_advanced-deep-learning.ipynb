{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/dsci572_header.png\" width=\"600\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 8: Advanced Convolutional Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture Learning Objectives\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Describe what few-shot learning is and how can it be useful\n",
    "\n",
    "- Describe what a generative adversarial network is and what they can be useful for"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, Subset\n",
    "from torchvision import transforms, datasets, utils, models\n",
    "from torchsummary import summary\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "from PIL import Image\n",
    "from statistics import mean\n",
    "\n",
    "plt.rcParams.update({'axes.grid': False})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few-Shot Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We have so far discussed how CNNs can be used to classify images\n",
    "\n",
    "- The assumption here is that we have access to relatively large datasets that contain 100s or 1000s of images per class\n",
    "- However, this is not always the case. More data is usually hard, or even impossible to obtain\n",
    "- Take the example of a **face recognition** system for a company's employees:\n",
    "\n",
    "  - Does it make sense to ask each employee to provide, say, 1000 images of their face so as to register them in the system?\n",
    "\n",
    "  - Even if we have 1000 images of each employee, we'd have to retrain the model every time a new employee is added to the system"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For cases like these, we can use a technique called **few-shot** learning. Instead of training the model on a new class of images, we use the similarities and differences of an image with other images to decide the class of that image.\n",
    "\n",
    "- Let me explain this with some examples (credit: most images in this section are adopted from this [website](https://zzaebok.github.io/machine_learning/FSL/) and [Youtube video](https://www.youtube.com/watch?v=hE7eGew4eeg)):\n",
    "\n",
    "For training a conventional CNN, we use a dataset like this:\n",
    "\n",
    "<img src=\"https://imgur.com/6MAisQL.png\" width=\"600\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we want to predict the class of a new image, that class should already exist in the training set of the CNN:\n",
    "\n",
    "<img src=\"https://imgur.com/H1rP0Aw.png\" width=\"600\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-But what if the class of the test image does not exist among the classes on which the CNN was trained?\n",
    "\n",
    "- This is the kind of problem we're interested in solving with few-shot learning\n",
    "\n",
    "- **Few-shot learning is about learning to classify a new test image, with only few examples of that new class**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here for example, rabbit class does not exist in the training set. It would be desirable to have a model that can learn to classify this **query** image, based on a few images of rabbits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://imgur.com/StTYXay.png\" width=\"600\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The dataset of images on which the model wasn't trained on is called the **support set**\n",
    "\n",
    "- This problem is called **k-way n-shot** learning, when we have **k classes** and **n samples per class** in the **support set**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://imgur.com/UsmR4Ow.png\" width=\"600\">\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Now comes the key idea of few-show learning:** instead of directly learning how to classify, learn how to find similarities between samples belonging to the same class, and differences between samples belonging to different classes\n",
    "\n",
    "- In other words, instead of **learning the classification itself**, we'd like to **learn how to learn the classification**!\n",
    "\n",
    "- This is why few-shot learning is said to be an example of **meta learning**\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Siamese Networks for Few-Shot Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As mentioned above, the goal is to learn **similarities** and **differences** between images in the same and different classes, rather than the corresponding classes\n",
    "\n",
    "- Therefore, it seems natural to think that a CNN could be used for feature extraction\n",
    "\n",
    "- An interesting model for few-shot learning is a Siamese (or twin) network ([image source](https://people.kth.se/~rosun/deep-learning/figures/siamese-arch.svg)):\n",
    "\n",
    "<img src=\"https://people.kth.se/~rosun/deep-learning/figures/siamese-arch.svg\" width=\"700\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The Siamese network is supposed to learn similarities and differences\n",
    "\n",
    "- This is why the first step is to **construct a new dataset from an existing one** as follows:\n",
    "\n",
    "<img src=\"img/pos_neg.png\" width=\"700\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Positive samples are pairs of images that belong to the same class\n",
    "\n",
    "- Negative samples are pairs of images that don't belong to the same class\n",
    "\n",
    "- The Siamese network is a parallel neural network architecture has two streams of images: one for each image in a pair of images, that form either a positive or negative sample\n",
    "\n",
    "- The two streams share exactly the same architecture and model parameters. This is why this particular architecture is called a **Siamese** or **twin** network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How does a Siamese network learn?**\n",
    "\n",
    "A Siamese network uses a particular type of loss function called **contrastive loss**, with the following form:\n",
    "\n",
    "$$\n",
    "(1-Y) \\left(D\\right)^2+(Y) \\left\\{\\max \\left(0, m-D\\right)\\right\\}^2\n",
    "$$\n",
    "\n",
    "where $Y$ is the label of the generated samples; 0 for similar images (positive sample), and 1 for dissimilar images (negative sample).\n",
    "\n",
    "- The contrastive loss tries to decrease distance between embeddings (feature vectors) of similar images, and to increase distance between embeddings of dissimilar images.\n",
    "\n",
    "- In other words, through the contrastive loss, a Siamese network tries to pull similar images together, and push dissimilar images away, in the embedding space"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I'll explain the implementation in the lecture (the code below is adopted from [here](https://github.com/harveyslash/Facial-Similarity-with-Siamese-Networks-in-Pytorch/blob/master/Siamese-networks-medium.ipynb) with some minor changes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetworkDataset(Dataset):\n",
    "    def __init__(self, imageFolderDataset, transform=None):\n",
    "        self.imageFolderDataset = imageFolderDataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        idx = np.random.choice(len(self.imageFolderDataset.imgs))\n",
    "        img0_tuple = self.imageFolderDataset.imgs[idx]\n",
    "\n",
    "        # 50% chance of images being in the same class\n",
    "        should_get_same_class = np.random.randint(0, 2)\n",
    "        if should_get_same_class:\n",
    "            while True:\n",
    "                # loop untill the same class is found\n",
    "                idx = np.random.choice(len(self.imageFolderDataset.imgs))\n",
    "                img1_tuple = self.imageFolderDataset.imgs[idx]\n",
    "                if img0_tuple[1] == img1_tuple[1]:\n",
    "                    break\n",
    "        else:\n",
    "            while True:\n",
    "                # loop untill a different class is found\n",
    "                idx = np.random.choice(len(self.imageFolderDataset.imgs))\n",
    "                img1_tuple = self.imageFolderDataset.imgs[idx]\n",
    "                if img0_tuple[1] != img1_tuple[1]:\n",
    "                    break\n",
    "\n",
    "        img0 = Image.open(img0_tuple[0])\n",
    "        img1 = Image.open(img1_tuple[0])\n",
    "\n",
    "        # convert to gray-scale\n",
    "        img0 = img0.convert(\"L\")\n",
    "        img1 = img1.convert(\"L\")\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img0 = self.transform(img0)\n",
    "            img1 = self.transform(img1)\n",
    "\n",
    "        return (\n",
    "            img0,\n",
    "            img1,\n",
    "            torch.from_numpy(\n",
    "                np.array([int(img1_tuple[1] != img0_tuple[1])], dtype=np.float32)\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cnn = nn.Sequential(\n",
    "\n",
    "            nn.Conv2d(1, 96, kernel_size=5,stride=2),\n",
    "            nn.BatchNorm2d(96),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(3, stride=2),\n",
    "            \n",
    "            nn.Conv2d(96, 64, kernel_size=3, stride=2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "\n",
    "            nn.Linear(3136, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Linear(128, 32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Linear(32, 2)\n",
    "        )\n",
    "        \n",
    "    def forward_each(self, x):\n",
    "        output = self.cnn(x)\n",
    "        output = self.fc(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        output1 = self.forward_each(input1)\n",
    "        output2 = self.forward_each(input2)\n",
    "\n",
    "        return output1, output2\n",
    "\n",
    "\n",
    "model = SiameseNetwork()\n",
    "summary(model, [(1, 128, 128,), (1, 128, 128,)]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(torch.nn.Module):\n",
    "    def __init__(self, margin=2.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        # Calculate the euclidian distance and calculate the contrastive loss\n",
    "        euclidean_distance = F.pairwise_distance(output1, output2, keepdim=True)\n",
    "\n",
    "        loss_contrastive = torch.mean(\n",
    "        (1 - label) * torch.pow(euclidean_distance, 2)\n",
    "        + (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2)\n",
    "        )\n",
    "        \n",
    "        return loss_contrastive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gridshow(img, text=None):\n",
    "    npimg = img.numpy()\n",
    "    plt.axis(\"off\")\n",
    "    if text:\n",
    "        plt.text(75, 8, text, style='italic',fontweight='bold',\n",
    "            bbox={'facecolor':'white', 'alpha':0.8, 'pad':10})\n",
    "        \n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_dataset = datasets.ImageFolder(root=\"data/faces/training\")\n",
    "\n",
    "transformation = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "siamese_dataset = SiameseNetworkDataset(\n",
    "    imageFolderDataset=folder_dataset, transform=transformation\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(siamese_dataset, shuffle=True, batch_size=8)\n",
    "\n",
    "example_batch = next(iter(train_loader))\n",
    "concatenated = torch.cat((example_batch[0], example_batch[1]), axis=0)\n",
    "\n",
    "gridshow(utils.make_grid(concatenated, nrow=8))\n",
    "print(example_batch[2].numpy().reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "\n",
    "model = SiameseNetwork().to(device)\n",
    "criterion = ContrastiveLoss(margin=3.0)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "siamese_dataset = SiameseNetworkDataset(imageFolderDataset=folder_dataset, transform=transformation)\n",
    "# siamese_dataset = Subset(siamese_dataset, range(500))\n",
    "\n",
    "train_loader = DataLoader(siamese_dataset, shuffle=True, batch_size=64)\n",
    "\n",
    "loss_history = [] \n",
    "\n",
    "for epoch in range(200):\n",
    "\n",
    "    for i, (img0, img1, label) in enumerate(train_loader, 0):\n",
    "\n",
    "        if device.type in ['cuda', 'mps']:\n",
    "            img0, img1, label = img0.to(device), img1.to(device), label.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output1, output2 = model(img0, img1)\n",
    "        loss_contrastive = criterion(output1, output2, label)\n",
    "        loss_contrastive.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 10 == 0 :\n",
    "            print(f\"Epoch {epoch}: Training batch loss = {loss_contrastive.item():g}\")\n",
    "            loss_history.append(loss_contrastive.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.loglog(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "folder_dataset_test = datasets.ImageFolder(root=\"data/faces/testing/\")\n",
    "siamese_dataset = SiameseNetworkDataset(imageFolderDataset=folder_dataset_test, transform=transformation)\n",
    "                                        \n",
    "# siamese_dataset = Subset(siamese_dataset, range(2000))\n",
    "\n",
    "test_dataloader = DataLoader(siamese_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# Take one image to test on\n",
    "dataiter = iter(test_dataloader)\n",
    "x0, _, _ = next(dataiter)\n",
    "\n",
    "if device.type in ['cuda', 'mps']: x0 = x0.to(device)\n",
    "\n",
    "for i in range(10):\n",
    "    _, x1, label2 = next(dataiter)\n",
    "    if device.type in ['cuda', 'mps']:\n",
    "        x1, labe12 = x1.to(device), label2.to(device)\n",
    "\n",
    "    concatenated = torch.cat((x0, x1), 0)\n",
    "    \n",
    "    output1, output2 = model(x0, x1)\n",
    "    euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "    gridshow(utils.make_grid(concatenated.cpu()), f'Distance: {euclidean_distance.item():.4g}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative Adversarial Networks (GANs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### What are GANs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "GANs are a type of neural network models that are used to generate new data, that is indistinguishable from the data that exists in a dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "For example, suppose that we have a dataset of 10,000 images. The question is: can we somehow generate images that are so real-looking that we can't tell if they are **real** (could potentially come from the dataset) or **fake** (generated by some algorithm)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Here, there aren't any real labels. We just want to be able to produce images that are as real-looking as possible; we don't classify them. This is why **GAN modeling** is regarded as an **unsupervised learning** task. In other words, we just need a bunch of images (or input data), no labels (or target data or outputs) would be required."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "GANs were invented in 2014 by Ian Goodfellow and colleagues (see the original paper [here](https://arxiv.org/abs/1406.2661)); and have been called \"_the most interesting idea in the last 10 years in ML_\" by Yann LeCun, Facebook’s AI research director."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now take a look at the following image:\n",
    "\n",
    "<img src=\"img/fake-face.jpeg\" width=\"400\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Believe it or not, **this is not a real person!**\n",
    "\n",
    "The image above is produced by a GAN that is trained on human faces. If you want to see more, visit [www.thispersondoesnotexist.com](www.thispersondoesnotexist.com). The website is connected to a GAN model living on the cloud, and each time the page refreshes, it generates a new image of a person who **does not exist!**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Structure of a GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "In this section, I describe how GANs work for image data, but remember that the idea of GANs is generalizable to any kind of data, not just images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Here is the visualization of the structure of a GAN:\n",
    "\n",
    "<img src=\"img/gan-1.png\" width=\"900\"><br>\n",
    "[(image source)](https://freecontent.manning.com/practical-applications-of-gans-part-1/)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "The structure of a GAN consists of a **discriminator** and a **generator**:\n",
    "\n",
    "- A discriminator is just a **typical CNN** that receives an image as the input, and generates a vector of probabilities of the input belonging to some class\n",
    "\n",
    "- A generator is an **inverted CNN** that receives a vector of random numbers and generates an image in the output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "The word \"adversarial\" comes from the fact that we actually have two networks battling each other:\n",
    "\n",
    "- The generator: tries to generate fake images that look as realistic as possible such that it can **fool** the discriminator\n",
    "\n",
    "- The discriminator: takes in real data and fake data and tries to correctly determine whether an input was real or fake"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**An analogy:**\n",
    "\n",
    "Think of the \"Generator\" as a new counterfeit artist trying to produce realistic-looking famous artworks to sell.\n",
    "\n",
    "The \"Discriminator\" is an art critic, trying to determine if a piece of art is \"real\" or \"fake\".\n",
    "\n",
    "At first, the \"Generator\" produces poor art-replicas which the \"Discriminator\" can easily tell are fake. But over time, the \"Generator\" learns ways to produce art that fools the \"Discriminator\". Eventually, the \"Generator\" becomes so good that the \"Discriminator\" can't tell if a piece of art is real or fake."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Convolution Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Convolution layers are used in the discriminator of a GAN to do a binary classification (real vs. fake). Their structure is very much the same as what we have seen so far in the course for CNNs.\n",
    "\n",
    "Convolution layers **downsample** input features. In other words, the goal of convolution layers is to go from **larger features (images)** to **smaller features (images)**.\n",
    "\n",
    "Here is an animation of how kernels are applied to input features:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"img/conv-padded.gif\" width=\"200\"><img src=\"img/conv-strided-padded.gif\" width=\"200\">\n",
    "<br>\n",
    "[(image source)](https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md#transposed-convolution-animations)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Transposed Convolution Layers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "This is the first time we see transposed convolution layers. These layers do the opposite of what convolution layers do; that is, instead of downsampling images, they upsample. Transposed convolutions are used in the generator of a GAN to generate a image from some random vector."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "The goal of transposed convolution layers is to go from **smaller features (images)** to **larger features (images)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"img/conv-trans-strided.gif\" width=\"200\"><img src=\"img/conv-trans-not-strided.gif\" width=\"200\"><br>\n",
    "[(image source)](https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md#transposed-convolution-animations)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "While downsampling seems very intuitive, upsampling might look like doing magic: we try to generate information (pixel values) that did not exist before. But it's practically not hard to do.\n",
    "\n",
    "We do something similar to what we did with convolution layers: we convolve (pass) the kernel over the inputs, and multiply each input element by all kernel elements. The resulting array will be part of the larger image:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"img/conv-trans-how.svg\" width=\"500\"><br>\n",
    "[(image source)](https://d2l.ai/chapter_computer-vision/transposed-conv.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"img/conv-trans-how2.svg\" width=\"500\"><br>\n",
    "[(image source)](https://d2l.ai/chapter_computer-vision/transposed-conv.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "If we repeat this operation in several layers, we can progressively increase the size of the input images. This is exactly what the generator does; it starts from some random noise, and progressively expands that into larger and larger images."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Training GANs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Training a GAN happens in two iterative phases:\n",
    "\n",
    "1. **Train the Discriminator:**\n",
    "\n",
    "    - Generate some fake images with the generator\n",
    "    \n",
    "    - Show the discriminator real images and fake images and get it to classify them correctly (a simple binary classification problem)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "2. **Train the Generator:**\n",
    "\n",
    "    - Generate fake images with the generator but label them as \"real\"\n",
    "    \n",
    "    - Pass these fake images through the discriminator, and ask it for its judgment, i.e. the probability of this image being real\n",
    "    - Pass this judgment to a loss function, and see how far it is from the ideal output. The ideal output is that the generator was so good that it has fooled the discriminator to give it the label of \"real\".\n",
    "    - Do backpropagation based on the gradients of this loss value to adjust the parameters of the generator, such that it can better and better fool the discriminator."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "3. **Repeat**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/gan-train.png\" width=\"600\"><br>\n",
    "[(image source)](https://sthalles.github.io/intro-to-gans/)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### PyTorch Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Alright, now's the time to implement a GAN in PyTorch. Since training GANs is a very resource-intensive job, we need to do our computations on a GPU. Here, I'll write the code **so that you can take this notebook and run it directly on [Kaggle](https://www.kaggle.com)**. If you want to run it on your own computer, you need to change the folder paths of the dataset that we're going to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f\"Using device: {device.type}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "For the purpose of demonstrating how training a GAN works in PyTorch, I have chosen to use the [_Face Recognition Dataset_](https://www.kaggle.com/stoicstatic/face-recognition-dataset) from Kaggle, which contains face images of celebrities.\n",
    "\n",
    "This dataset contains two folders: `Face Dataset` and `Extracted Faces`. We'll use the images in `Extracted Faces`, which are already `128x128` pixels. Therefore, there is no need to resize them, which is why I've commented out `transforms.Resize(IMAGE_SIZE)` in the code below. This speeds up the computations significantly, as resizing is done on CPU and it would have been the bottle-neck of our computations. Fortunately, we don't need to do this here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_DIR = \"../input/face-recognition-dataset/Extracted Faces\"\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "IMAGE_SIZE = (128, 128)\n",
    "\n",
    "data_transforms = transforms.Compose([\n",
    "#     transforms.Resize(IMAGE_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "dataset = datasets.ImageFolder(root=DATA_DIR, transform=data_transforms)\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        \n",
    "# Plot samples\n",
    "sample_batch = next(iter(data_loader))\n",
    "plt.figure(figsize=(10, 8)); plt.axis(\"off\"); plt.title(\"Sample Training Images\")\n",
    "plt.imshow(np.transpose(utils.make_grid(sample_batch[0], padding=1, normalize=True), (1, 2, 0)));\n",
    "\n",
    "print(f'Size of dataset: {len(data_loader) * BATCH_SIZE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Example output:\n",
    "\n",
    "<img src=\"img/faces.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Creating the Generator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "The generator takes in a random vector called **latent vector**, which can be thought of as a `1x1` pixel image having an arbitrary number of **channels** (specified in the code by `LATENT_SIZE`).\n",
    "\n",
    "Through the generator, we pass this latent vector through the **deconvolution** layers (known as **transposed convolution** layers in PyTorch), and progressively expand its size, such that in the output we'll have an image similar in dimensions to the images in our dataset. Here for example, our images in the dataset are `128x128`, so the goal of the generator is to start from an image of `1x1` pixel and generate an image of `128x128` pixels.\n",
    "\n",
    "**Details:**\n",
    "\n",
    "- In the following code, I've used `nn.BatchNorm2d()` for all layers, `nn.LeakyReLU()` as activation for intermediate layers, and `nn.Tanh()` as activation for the output of the generator. These are suggested to be used based on empirical evidence in training GANs.\n",
    "\n",
    "- We usually do in-place modification of tensors in `nn.LeakyReLU()` by setting `inplace=True` to save some memory.\n",
    "- We set `bias=False` because the batch normalization layer contains a bias term, so we don't want to do it twice.\n",
    "\n",
    "Note that we mainly play with the strides to progressively expand the size of the input latent vector.\n",
    "\n",
    "Here is the code for the generator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \n",
    "    def __init__(self, LATENT_SIZE):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        self.main = nn.Sequential(\n",
    "            \n",
    "            # input dim: [-1, LATENT_SIZE, 1, 1]\n",
    "            \n",
    "            nn.ConvTranspose2d(LATENT_SIZE, 1024, kernel_size=4, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # output dim: [-1, 1024, 4, 4]\n",
    "\n",
    "            nn.ConvTranspose2d(1024, 512, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # output dim: [-1, 512, 8, 8]\n",
    "\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # output dim: [-1, 256, 16, 16]\n",
    "\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # output dim: [-1, 128, 32, 32]\n",
    "            \n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # output dim: [-1, 64, 64, 64]\n",
    "\n",
    "            nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(3),\n",
    "            \n",
    "            # output dim: [-1, 3, 128, 128]\n",
    "            \n",
    "            nn.Tanh()\n",
    "            \n",
    "            # output dim: [-1, 3, 128, 128]\n",
    "        )\n",
    "        \n",
    "    def forward(self, input):\n",
    "        output = self.main(input)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The details of how exactly a transposed convolution layer works in PyTorch can be fairly confusing at first, but here's some further remarks to help you feel more comfortable with it:\n",
    "\n",
    "The parameters `stride` and `padding` in `nn.ConvTranspose2d` are (unfortunately?) not what we’re used to in using `nn.Conv2d`. For example, `stride=2` doesn’t mean that the kernel in the transposed convolution moves in steps of 2 pixels each time. These parameters are, instead, designed such that if you use the same stride and padding for a `ConvTranspose2d` as in a `Conv2d`, and apply it on the output of the that `Conv2d`, it will give you an image of the same shape (but not the same pixel values). In other words, if\n",
    "\n",
    "```python\n",
    "out_conv = Conv2d(img, stride=s, padding=p)\n",
    "x = out.shape\n",
    "```\n",
    "\n",
    "and\n",
    "\n",
    "```python\n",
    "out_convT = ConvTranspose2d(out_conv, stride=s, padding=p)\n",
    "y = out_convT.shape\n",
    "```\n",
    "\n",
    "then\n",
    "\n",
    "```python\n",
    "x == y --> True\n",
    "```\n",
    "\n",
    "In fact, the `stride` and `padding` work this way to make writing code easier.\n",
    "\n",
    "If you’re wondering about the mechanics of computing a transposed convolution in various scenarios, **make sure to check out [this blog post](https://numbersmithy.com/understanding-transposed-convolutions-in-pytorch/)**—the author has done an excellent job of explaining all the relevant details of `nn.ConvTranspose2d` and has some very useful diagrams as well."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Creating the Discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "As discussed before, this is a conventional CNN that receives an image (`128x128` in our case here) and outputs the probability of this image belonging to some certain class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.main = nn.Sequential(\n",
    "        \n",
    "            # input dim: [-1, 3, 128, 128]\n",
    "            \n",
    "            nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # output dim: [-1, 64, 64, 64]\n",
    "\n",
    "            nn.Conv2d(64, 64, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # output dim: [-1, 64, 32, 32]\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # output dim: [-1, 128, 16, 16]\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # output dim: [-1, 256, 8, 8]\n",
    "\n",
    "            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # output dim: [-1, 512, 4, 4]\n",
    "\n",
    "            nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=0),\n",
    "            \n",
    "            # output dim: [-1, 1, 1, 1]\n",
    "\n",
    "            nn.Flatten(),\n",
    "            \n",
    "            # output dim: [-1]\n",
    "\n",
    "            nn.Sigmoid()\n",
    "            \n",
    "            # output dim: [-1]\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.main(input)\n",
    "        return output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Instantiating and Initializing our GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let's create the discriminator and generator objects, as well as the loss function and optimizers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "LATENT_SIZE = 200\n",
    "\n",
    "generator = Generator(LATENT_SIZE)\n",
    "discriminator = Discriminator()\n",
    "\n",
    "generator.to(device)\n",
    "discriminator.to(device)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "optimizerG = optim.Adam(generator.parameters(), lr=0.001, betas=(0.5, 0.999))\n",
    "optimizerD = optim.Adam(discriminator.parameters(), lr=0.001, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "We explored how the starting point of the optimization can affect the final results. It is recommended that to initialize the weights of a GAN with values obtained randomly from a normal distribution. In PyTorch, we can define the initialization function as we like and apply it to the model parameters using the `.apply()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "        \n",
    "generator.apply(weights_init)\n",
    "discriminator.apply(weights_init);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Training our GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the following cell to keep track of how a fixed noise (latent vector) is transformed to a generated image in each epoch. We will see that the generations become better and better throughout the epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img_list = []\n",
    "fixed_noise = torch.randn(BATCH_SIZE, LATENT_SIZE, 1, 1).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, here is the training loop (you can also put everything inside a function):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 50\n",
    "\n",
    "print('Training started:\\n')\n",
    "\n",
    "D_real_epoch, D_fake_epoch, loss_dis_epoch, loss_gen_epoch = [], [], [], []\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    \n",
    "    D_real_iter, D_fake_iter, loss_dis_iter, loss_gen_iter = [], [], [], []\n",
    "    \n",
    "    for real_batch, _ in data_loader:\n",
    "\n",
    "        # STEP 1: train discriminator\n",
    "        # ==================================\n",
    "        # Train with real data\n",
    "        discriminator.zero_grad()\n",
    "        \n",
    "        real_batch = real_batch.to(device)\n",
    "        real_labels = torch.ones((real_batch.shape[0],), dtype=torch.float).to(device)\n",
    "        \n",
    "        output = discriminator(real_batch).view(-1)\n",
    "        loss_real = criterion(output, real_labels)\n",
    "        \n",
    "        # Iteration book-keeping\n",
    "        D_real_iter.append(output.mean().item())\n",
    "        \n",
    "        # Train with fake data\n",
    "        noise = torch.randn(real_batch.shape[0], LATENT_SIZE, 1, 1).to(device)\n",
    "        \n",
    "        fake_batch = generator(noise)\n",
    "        fake_labels = torch.zeros_like(real_labels)\n",
    "        \n",
    "        output = discriminator(fake_batch.detach()).view(-1)\n",
    "        loss_fake = criterion(output, fake_labels)\n",
    "        \n",
    "        # Update discriminator weights\n",
    "        loss_dis = loss_real + loss_fake\n",
    "        loss_dis.backward()\n",
    "        optimizerD.step()\n",
    "        \n",
    "        # Iteration book-keeping\n",
    "        loss_dis_iter.append(loss_dis.mean().item())\n",
    "        D_fake_iter.append(output.mean().item())\n",
    "        \n",
    "        # STEP 2: train generator\n",
    "        # ==================================\n",
    "        generator.zero_grad()\n",
    "        output = discriminator(fake_batch).view(-1)\n",
    "        loss_gen = criterion(output, real_labels)\n",
    "        loss_gen.backward()\n",
    "        \n",
    "        # Book-keeping\n",
    "        loss_gen_iter.append(loss_gen.mean().item())\n",
    "        \n",
    "        # Update generator weights and store loss\n",
    "        optimizerG.step()\n",
    "        \n",
    "    print(f\"Epoch ({epoch + 1}/{NUM_EPOCHS})\\t\",\n",
    "          f\"Loss_G: {mean(loss_gen_iter):.4f}\",\n",
    "          f\"Loss_D: {mean(loss_dis_iter):.4f}\\t\",\n",
    "          f\"D_real: {mean(D_real_iter):.4f}\",\n",
    "          f\"D_fake: {mean(D_fake_iter):.4f}\")\n",
    "    \n",
    "    # Epoch book-keeping\n",
    "    loss_gen_epoch.append(mean(loss_gen_iter))\n",
    "    loss_dis_epoch.append(mean(loss_dis_iter))\n",
    "    D_real_epoch.append(mean(D_real_iter))\n",
    "    D_fake_epoch.append(mean(D_fake_iter))\n",
    "    \n",
    "    # Keeping track of the evolution of a fixed noise latent vector\n",
    "    with torch.no_grad():\n",
    "        fake_images = generator(fixed_noise).detach().cpu()\n",
    "        img_list.append(utils.make_grid(fake_images, normalize=True, nrows=10))\n",
    "        \n",
    "print(\"\\nTraining ended.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Example output:\n",
    "\n",
    "```\n",
    "Epoch (1/50)\t Loss_G: 16.1643 Loss_D: 2.7680\t D_real: 0.8808 D_fake: 0.1760\n",
    "Epoch (2/50)\t Loss_G: 5.1395 Loss_D: 1.2464\t D_real: 0.7797 D_fake: 0.2442\n",
    "Epoch (3/50)\t Loss_G: 2.3695 Loss_D: 1.3886\t D_real: 0.6162 D_fake: 0.3758\n",
    "Epoch (4/50)\t Loss_G: 2.3272 Loss_D: 1.3902\t D_real: 0.6065 D_fake: 0.3902\n",
    "Epoch (5/50)\t Loss_G: 2.4110 Loss_D: 1.2522\t D_real: 0.6205 D_fake: 0.3870\n",
    "Epoch (6/50)\t Loss_G: 2.6082 Loss_D: 1.2582\t D_real: 0.6301 D_fake: 0.3727\n",
    "Epoch (7/50)\t Loss_G: 2.3240 Loss_D: 1.2784\t D_real: 0.6152 D_fake: 0.3882\n",
    "Epoch (8/50)\t Loss_G: 2.3167 Loss_D: 1.3681\t D_real: 0.6032 D_fake: 0.3983\n",
    "Epoch (9/50)\t Loss_G: 2.3370 Loss_D: 1.2700\t D_real: 0.6068 D_fake: 0.3959\n",
    "Epoch (10/50)\t Loss_G: 2.3772 Loss_D: 1.2815\t D_real: 0.6077 D_fake: 0.3872\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Visualizing Training Progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "The following plots will help you see how the loss values of the generator and the discriminator, as well as the probabilities generated by the discriminator on real and fake images evolve during the training of our GAN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(np.array(loss_gen_epoch), label='loss_gen')\n",
    "plt.plot(np.array(loss_dis_epoch), label='loss_dis')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Example output:\n",
    "\n",
    "<img src=\"img/loss_epoch.png\" width=\"350\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(np.array(D_real_epoch), label='D_real')\n",
    "plt.plot(np.array(D_fake_epoch), label='D_fake')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Example output:\n",
    "\n",
    "<img src=\"img/D_epoch.png\" width=\"350\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "The following code cells help to see the evolution of one fixed noise vector throughout the epochs. The generator is applied on this fixed random noise in each epoch, and the results are saved as batches of generated images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ims = [[plt.imshow(np.transpose(i,(1, 2, 0)), animated=True)] for i in img_list[::1]]\n",
    "ani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)\n",
    "ani.save('GAN.gif', writer='imagemagick', fps=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "HTML(ani.to_jshtml()) # run this in a new cell to produce the below animation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "These are my results after running the GAN model for around 100 epochs:\n",
    "\n",
    "<img src=\"img/GAN.gif\" width=\"600\"><br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** You might have noticed the checker-board patterns that appear in the generated images, especially early in the training process. This is a known issue with transposed convolutions. This problem and potential solutions are discussed in great detail in [this article](https://distill.pub/2016/deconv-checkerboard/)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "dsci572",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "f821000d0c0da66e5bcde88c37d59c8e0de03b40667fb62009a8148ca49465a0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
